<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[股票和期货的日K线合成周/月K线]]></title>
    <url>%2F2019%2F09%2F12%2Fpandas-dataframe-conv-stock-kline%2F</url>
    <content type="text"><![CDATA[生成的周/月K线总是按周期的最后一日显示,如下方的模拟数据,输出了2019-09-13日,但这一天是中秋节,并不是交易日,按照一般股票软件展示是2019-09-12,所以为了这个问题折腾了一两天. 模拟数据:1234567891011121314import pandas as pdimport numpy as np# A股交易日数组dates = [ '2019-08-23', '2019-08-26', '2019-08-27', '2019-08-28', '2019-08-29', '2019-08-30', '2019-09-02', '2019-09-03', '2019-09-04', '2019-09-05', '2019-09-06', '2019-09-09', '2019-09-10', '2019-09-11', '2019-09-12', '2019-09-16', '2019-09-17']data = np.random.randint(low=1, high=5, size=[len(dates), 4]).tolist()df = pd.DataFrame(data, pd.to_datetime(dates), ['open', 'high', 'low', 'close'])# 合成K线_df = df.groupby(pd.Grouper(freq='W-FRI')).agg(dict(open='first', high='max', low='min', close='last'))print(_df.tail()) 期待结果: 123452019-08-23 2 2 4 12019-08-30 4 4 1 12019-09-06 4 3 2 42019-09-12 2 4 1 12019-09-17 4 4 2 2 实际输出: 123452019-08-23 2 2 4 12019-08-30 4 4 1 12019-09-06 4 3 2 42019-09-13 2 4 1 12019-09-20 4 4 2 2 解决方法:将date保留,利用agg里获取周期内的最后一个日期,这样就能保留周期内最后一日的正确日期. 代码: 1234567891011121314151617import pandas as pdimport numpy as npdates = [ '2019-08-23', '2019-08-26', '2019-08-27', '2019-08-28', '2019-08-29','2019-08-30', '2019-09-02', '2019-09-03', '2019-09-04', '2019-09-05', '2019-09-06', '2019-09-09', '2019-09-10', '2019-09-11', '2019-09-12', '2019-09-16', '2019-09-17']data = np.random.randint(low=1, high=5, size=[len(dates), 4]).tolist()df = pd.DataFrame(data, pd.to_datetime(dates), ['open', 'high', 'low', 'close'])df['date'] = datesprint(df.tail(10))# 这里是合成代码_df = df.groupby(pd.Grouper(freq='W-FRI')).agg(dict(open='first', high='max', low='min', close='last', date='last'))_df.set_index('date', inplace=True)print(_df.tail()) 实际输出: 123452019-08-23 2 1 4 42019-08-30 3 4 1 42019-09-06 3 4 1 32019-09-12 2 4 1 42019-09-17 3 4 1 1 总结:实际上一开始没想过来,总是以为时间作为index进行分组, 其实还能跟其他列一样进行聚合,换个方向思考会有不同的解决方案. ps:当一个股票停牌一段时间,然后复牌,那日K是跟正常的交易日历不一样的,如美的集团000333在2019-05-08到2019-05-21是停牌的,也许这种方式跟正常的股票不一样]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pandas</tag>
        <tag>股票</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以豆瓣电影为例,带你入门scrapy爬虫开发]]></title>
    <url>%2F2019%2F07%2F09%2Fscrapy-doubanmovie%2F</url>
    <content type="text"><![CDATA[最近一时冲动开通了一年某视频网站VIP会员,我却不知道有什么值得去看的,故看看一些经典电影吧,所以去豆瓣电影搜一波评分高的来作参考.在娱乐的同时也不能放松学习,本文将讲解python知名爬虫框架scrapy开发豆瓣电影信息爬虫.开发环境: ubuntu + python3 磨刀不误砍柴工,先搞定开发环境:安装必要依赖: 1sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev 创建python3虚拟环境: 12pip install virtualenv virtualenv -ppython3 .venv source .venv/bin/activate 安装scrapy: 1pip install scrapy 安装完后,来验证一下是否安装成功(有版本信息即成功): 1scrapy version 一般来说执行以上命令就完成了,但每个人的环境都不一样,如果有报错,请参考官方文档:https://docs.scrapy.org/en/latest/intro/install.html或者使用anaconda,如果pip安装速度太慢的话,请考虑使用pip国内源,这个网上很多教程. 善用工具之创建项目: 使用scrapy命令生成一个标准项目,并进入项目根目录: 123456 scrapy startproject doubanmovie cd doubanmovie ``` 生成第一个spider文件:```bash scrapy genspider douban movie.douban.com 使用tree查看整个项目结构,请战略性忽略文件夹__pycache__下的内容: 1234567891011121314151617 .├── doubanmovie│ ├── __init__.py # 包定义│ ├── items.py # 模型│ ├── middlewares.py # 中间件│ ├── pipelines.py # 管道│ ├── __pycache__ # 战略性忽略│ │ ├── __init__.cpython-36.pyc│ │ └── settings.cpython-36.pyc│ ├── settings.py # 配置文件│ └── spiders # spider(蜘蛛)文件夹,可以定义多个spider│ ├── douban.py # 生成的spider(蜘蛛)文件│ ├── __init__.py # 包定义│ └── __pycache__ # 战略性忽略│ ├── douban.cpython-36.pyc│ └── __init__.cpython-36.pyc└── scrapy.cfg # scrapy运行的配置文件 分析网站,然后编码:用浏览器(推荐使用chrome/firefox)打开豆瓣电影,点击网页上的选电影,然后就能看到我们要即将要爬的数据(见下图):电影类型,电影名称,电影评分. 首先得分析网页源码,右键查看源代码,搜索第一部电影的名字恶人传,发现搜不到内容,那么就判断该网页的电影数据是动态加载的,也就是常见的AJAX,那么我们打开浏览器开发者工具(F12), 切换到netWork/网络后刷新下当前网页,分析到两个请求地址: 电影类型: 1https://movie.douban.com/j/search_tags?type=movie&amp;source= 电影列表: 1https://movie.douban.com/j/search_subjects?type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0 电影列表请求地址参数表: 参数 值 type movie tag 热门(通过URL解码得出) sort recommend page_limit 展示电影的数量 page_start 从第几条记录开始 很明显,先获取到了所有的电影类型,然后每个类型作为参数拼凑到电影列表中,就能请求到该电影类型下的数据,接下来就开始写代码.编辑items.py,虽然可以直接用dict接收数据,但规范一点还是写个item吧. 12345678# -*- coding: utf-8 -*-import scrapyclass DoubanmovieItem(scrapy.Item): name = scrapy.Field() # 电影名字 rate = scrapy.Field() # 电影评分 tag = scrapy.Field() # 电影类型 编辑douban.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: utf-8 -*-# 引用python自带的jsonimport jsonimport scrapy# 引用我们定义好的item模型from ..items import DoubanmovieItemclass DoubanSpider(scrapy.Spider): # 这个spider(蜘蛛的名字) name = 'douban' # 不在此允许范围内的域名就会被过滤,不会进行爬取,其实这个可要可不要 allowed_domains = ['movie.douban.com'] # 这是入口 start_urls = ['https://movie.douban.com/j/search_tags?type=movie&amp;source='] # 定义电影详情url movie_info_url = 'https://movie.douban.com/j/search_subjects?' \ 'type=movie&amp;tag=&#123;tag&#125;&amp;sort=recommend&amp;page_limit=&#123;page_limit&#125;&amp;page_start=&#123;page_start&#125;' # 从start_urls获取第一个url访问后会进入这个方法 def parse(self, response): # body_as_unicode能尽量返回文本是unicode的,比较好处理 body = response.body_as_unicode() # 字符串转json obj = json.loads(body) tags = obj.get('tags', []) for tag in tags: cur_page = 1 # 定义meta,可以传递参数到下一个方法中 meta = dict(tag=tag, # 电影类型 page_limit=20, # 展示电影的数量 page_start=(cur_page - 1) * 20, # 从第几条记录开始 cur_page=cur_page ) # 生成每个类型下的第一页的电影列表url first_page_url = self.movie_info_url.format(**meta) yield scrapy.Request(url=first_page_url, callback=self.parse_movie_list_page, meta=meta) # 解析电影列表 def parse_movie_list_page(self, response): # parse里定义的meta meta = response.meta body = response.body_as_unicode() obj = json.loads(body) subjects = obj.get('subjects', []) # 将列表里的电影信息提取出来 for movie in subjects: item = DoubanmovieItem() item['name'] = movie['title'] item['rate'] = movie['rate'] item['tag'] = meta['tag'] yield item # 进行翻页请求,本文只作参考,所以就爬前三页内容后停止 cur_page = meta['cur_page'] if cur_page &lt; 3: meta['cur_page'] = cur_page + 1 meta['page_start'] = (meta['cur_page'] - 1) * 20 next_page_url = self.movie_info_url.format(**meta) # 这里的回调还是本方法,因为格式都没有变动,只是内容变了 yield scrapy.Request(url=next_page_url, callback=self.parse_movie_list_page, meta=meta) 修改settings.py: 1234# USER_AGENT,模拟浏览器UA,可以自行修改成其他的USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'# 是否遵守ROBOTSTXTROBOTSTXT_OBEY = False 牛刀小试之运行项目:在项目根目录下,控制台输入命令(得先激活虚拟环境):scrapy crawl douban -o movie.json,如无意外便能看到一堆信息输出,在底部看到&#39;item_scraped_count&#39;: 1020,说明爬到了1020条数据,保存在movie.json里,大家可以打开这个文件看看是否真的有1020条电影数据信息. 到这里就说明这个爬虫开发完成了,查看完整代码请查看github:https ://github.com/luojunhui/doubanmovie 写在最后:也许过一段时间,网页发生改动,到时候就需要自行分析网页了,本文仅仅是抛砖引玉,scrapy更多操作等着你挖掘!]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot统一异常拦截]]></title>
    <url>%2F2019%2F05%2F21%2Fspringboot-excption-handler-demo%2F</url>
    <content type="text"><![CDATA[起因: 公司某同事把我拉进了一个钉钉群,此群自动把生产环境的Spring项目异常信息通过钉钉机器人发出来. 之后每天一看消息,全部都是”未知异常”,还每天500+条,一口老血吐出来… 长期反馈给同事/上级无果后,打算自己研究下怎么获取具体的异常信息,好让负责这个功能的同事去修改,这样下来总能减少异常报错数量吧… 寻求解决之道:万事开头难,先搜下有没有解决方案吧,发现Error Handling for REST with Spring提出了有4个方案@ExceptionHandler,HandlerExceptionResolver,@ControllerAdvice,ResponseStatusException,结合当前公司项目情况,选用@ControllerAdvice里的@ExceptionHandler作为全局异常拦截. Coding:首先建立一个基础的Spring Boot web项目,作为一个合格的CV大师,一顿Ctrl+CCtrl+V后,项目已建好. 先写个controller,分别定义get和post接口. 123456789101112131415@RestController@RequestMapping("/api")public class ApiController &#123; @PostMapping(value = "div") public ResultVO div(@RequestBody MyModel obj) &#123; DivModel div = new DivModel(obj.getA(), obj.getB(), obj.getA() / obj.getB()); return new ResultVO("0000", null, div); &#125; @GetMapping(value = "div1") public ResultVO div1(MyModel obj) &#123; DivModel div = new DivModel(obj.getA(), obj.getB(), obj.getA() / obj.getB()); return new ResultVO("0000", null, div); &#125;&#125; 再写个拦截器 12345678910111213@ExceptionHandler(&#123;Exception.class&#125;)public ResponseEntity&lt;ResultVO&gt; errorHandler(Exception e) &#123; System.out.println("拦截到异常..."); System.out.println("请求地址:" + request.getRequestURL()); System.out.println("请求方式:" + request.getMethod()); String param = request.getQueryString(); if (!StringUtils.isEmpty(param)) &#123; System.out.println("URL参数:" + param); &#125; String fullStackTrace = ExceptionUtils.getFullStackTrace(e); System.out.println("堆栈信息:" + fullStackTrace); return new ResponseEntity&lt;ResultVO&gt;(new ResultVO("1111", "异常" + e.getMessage(), null), HttpStatus.INTERNAL_SERVER_ERROR);&#125; 然后运行项目试试两个接口(当被除数为0时会抛出异常).get:http://localhost:8080/api/div1?a=10&amp;b=0返回 1&#123;"code":"1111","msg":"异常/ by zero"&#125; 再看控制台输出 1234567拦截到异常...请求地址:http://localhost:8080/api/div1请求方式:GETURL参数:a=10&amp;b=0堆栈信息:java.lang.ArithmeticException: / by zero at cn.junhui.demo.controller.ApiController.div1(ApiController.java:25) ... 好,get能获取到异常信息和参数,再来试试post请求. 返回的json同上,但控制台输出就不一样了,少了参数!!! 12345拦截到异常...请求地址:http://localhost:8080/api/div请求方式:POST堆栈信息:java.lang.ArithmeticException: / by zero at cn.junhui.demo.controller.ApiController.div(ApiController.java:19) 由于本公司的项目接口都使用post,所以获取参数就换成request.getParameterMap(); 12Map&lt;String, String[]&gt; param = request.getParameterMap();param.forEach((key, value) -&gt; System.out.printf("%s:%s\n", key,value[0])); 和get效果一样,但post依然没有拿到参数,在断点调试中可以看到param是空的Map,因为请求体已经读过一次了,所以在其他地方读取就没有了. 解决:到google一下这个问题,发现stackoverflow上有解决方案. 接下来又是CV操作后再来看效果. 123456789拦截到异常...请求地址:http://localhost:8080/api/div请求方式:POST请求参数:&#123; &quot;a&quot;:10, &quot;b&quot;:0&#125;堆栈信息:java.lang.ArithmeticException: / by zero at cn.junhui.demo.controller.ApiController.div(ApiController.java:19) 终于能看到参数了.本次任务完成!最后附上demo完整代码]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
</search>
